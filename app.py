import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, BertForSequenceClassification
from peft import PeftModel
import re
import mysql.connector
from mysql.connector import Error
import datetime as dt
import torch
from kobert_tokenizer import KoBERTTokenizer
import random

########### DB ì„¸íŒ… ###############

x = dt.datetime.now()
date = x.strftime("%Y-%m-%d")
time = x.strftime("%H:%M:%S")

# DB ì—°ê²° í›„ ë°ì´í„° ì €ì¥
def db_connect():
    try:
        # DB ì—°ê²°
        conn = mysql.connector.connect(host = '127.0.0.1', database = 'testdb', user = 'coco', password = 'big5')
        if conn.is_connected():

            # ì»¤ì„œ ìƒì„±
            cursor = conn.cursor()
            
            # ë°ì´í„° ì €ì¥
            query = '''INSERT INTO test(question, answer, date, time) VALUES (%s, %s, %s, %s);'''
            input = (user_input, response, date, time)
            cursor.execute(query, input)
            
            # ì‘ì—… ì •ìƒ ì²˜ë¦¬
            conn.commit()

    # ì—ëŸ¬ ì¶œë ¥
    except Error as e :
        print('DB ì—ëŸ¬ ë°œìƒ', e)

    # DB ì—°ê²° í›„ ë°ì´í„° ì €ì¥ì´ ì™„ë£Œë˜ë©´ ì»¤ì„œì™€ ì»¤ë„¥ì…˜ ì¢…ë£Œ
    finally:
        cursor.close()
        conn.close()



######## ë¬´ê´€í•œ ë‹µë³€ ì„¸íŒ… (kobert)########
def check_answer(user_input):

    # ê°™ì€ ëª¨ë¸ êµ¬ì¡°ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=2)

    model_path = '/home/user/cocobot/python_question_model.pth'
    model.load_state_dict(torch.load(model_path, map_location=torch.device('cuda')))

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')

    max_len = 512

    # ì§ˆë¬¸ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    inputs = tokenizer.encode_plus(
        user_input,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt',
        truncation=True
    )

    # ëª¨ë¸ë¡œ ì§ˆë¬¸ ì¶”ë¡ 
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ í•˜ì§€ ì•ŠìŒ
        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])
        prediction = torch.argmax(outputs.logits, dim=1)

    # ê²°ê³¼ ë°˜í™˜
    predicted_label = prediction.cpu().numpy()[0]

    return predicted_label

# if ë¬´ê´€í•œ ì§ˆë¬¸ì¼ ê²½ìš° ë‹µë³€ì…‹
with open('./answer.txt', 'r', encoding='utf-8') as file:
    answers = file.read().split('\n')

############# page ì„¸íŒ… ì‹œì‘ ################

st.set_page_config(page_title="My Llama2 Chatbot")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "sidebar_history" not in st.session_state:
    st.session_state.sidebar_history = []
if "full_history" not in st.session_state:
    st.session_state.full_history = []
if "session_active" not in st.session_state:
    st.session_state.session_active = False

st.title(":koala: Coala")
st.title(':blue_book: íŒŒì´ì¬ì— ëŒ€í•´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”.')


##### ì‚¬ì´ë“œë°” #######
st.sidebar.title("ì§ˆë¬¸ ì´ë ¥ :book:")
st.sidebar.write("---")

# "ìƒˆë¡œìš´ ì§ˆë¬¸í•˜ê¸°" ë²„íŠ¼ ë¡œì§
if st.sidebar.button("ìƒˆë¡œìš´ ì§ˆë¬¸í•˜ê¸°â•"):

    # í˜„ì¬ ëŒ€í™” ì´ë ¥ì´ ì¡´ì¬í•˜ê³ , ì„¸ì…˜ì´ ë³µì›ëœ ìƒíƒœê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
    if st.session_state.chat_history and not st.session_state.get('restored_session', False):
        
        # ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ ì°¾ê¸°
        first_user_question = next((msg for msg in st.session_state.chat_history if msg["role"] == "user"), None)
        
        if first_user_question:
            
            # ì‚¬ì´ë“œë°” ì´ë ¥ì— ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
            st.session_state.sidebar_history.append(first_user_question)
        
        # í˜„ì¬ ëŒ€í™” ì´ë ¥ì„ full_historyì— ì €ì¥
        st.session_state.full_history.append(st.session_state.chat_history.copy())
    
    # ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘
    st.session_state.chat_history = []
    
    # ì„¸ì…˜ì´ ë³µì›ëœ ìƒíƒœë¥¼ Falseë¡œ ì„¤ì •
    st.session_state.restored_session = False

# ì‚¬ì´ë“œë°”ì˜ ì§ˆë¬¸ ì´ë ¥ í‘œì‹œ ë° ë³µêµ¬
for idx, question in enumerate(st.session_state.sidebar_history):
    if st.sidebar.button(f"{idx + 1}. {question['content']}"):
        
        # ì„ íƒëœ ì„¸ì…˜ì˜ ëŒ€í™” ì´ë ¥ ë¡œë“œ
        st.session_state.chat_history = st.session_state.full_history[idx]
        
        # ì„¸ì…˜ì´ ë³µì›ëœ ìƒíƒœë¥¼ Trueë¡œ ì„¤ì •
        st.session_state.restored_session = True

# BitsAndBytesConfig ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_8bit=False,
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=None,
    llm_int8_enable_fp32_cpu_offload=False,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
    bnb_4bit_compute_dtype="float16",
)

base_model_path = "TinyPixel/CodeLlama-7B-Python-bf16-sharded"
peft_model_path = "kgyalice/llama2_peft"

@st.cache_resource
def load_model(base_model_path, peft_model_path):
    model = AutoModelForCausalLM.from_pretrained(base_model_path, quantization_config=bnb_config, device_map={"":0})
    model = PeftModel.from_pretrained(model, peft_model_path)
    return model

@st.cache_data
def load_tokenizer(base_model_path):
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    return tokenizer

model = load_model(base_model_path, peft_model_path)
tokenizer = load_tokenizer(base_model_path)

# ê¸°ì¡´ì˜ prompt ë° gen í•¨ìˆ˜ ì •ì˜
prompt = "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: "

def gen(x):
    q = prompt % (x,)
    gened = model.generate(
        **tokenizer(
            q,
            return_tensors='pt',
            return_token_type_ids=False
        ).to('cuda'),
        max_new_tokens=512,
        early_stopping=True,
        do_sample=False,
    )
    return tokenizer.decode(gened[0]).replace(q, "")

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì€ ì—¬ê¸°ì— ì…ë ¥í•´ ì£¼ì„¸ìš”.")

# ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•œ ê²½ìš°
if user_input:
    new_message = {"role": "user", "content": user_input}
    st.session_state.chat_history.append(new_message)

    # ëŒ€í™” ì´ë ¥ í‘œì‹œ (ì‚¬ìš©ì ì§ˆë¬¸ í¬í•¨)
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"], avatar="ğŸ¨" if message["role"] == "chatbot" else None):
            st.write(message["content"])

    # ì‘ë‹µ ìƒì„± ì‹œì‘ ì „ ìŠ¤í”¼ë„ˆ êµ¬í˜„
    with st.spinner("ë‹µë³€ ì¤‘ì´ì—ìš”.."):

        # íŒŒì´ì¬ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸
        if check_answer(user_input) == 0:
            response = random.choice(answers)

        # íŒŒì´ì¬ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸
        else:
            response = gen(user_input)
            response = re.sub(r'</?s>', '', response)

    db_connect()

    new_response = {"role": "chatbot", "content": response}
    st.session_state.chat_history.append(new_response)

    # ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ í™”ë©´ ê°±ì‹ 
    st.rerun()

# ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ì´ì „ ë©”ì‹œì§€ë“¤ í‘œì‹œ
if not user_input:
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"], avatar="ğŸ¨" if message["role"] == "chatbot" else None):
            st.write(message["content"])